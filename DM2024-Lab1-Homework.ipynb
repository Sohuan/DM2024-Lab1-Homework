{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 楊碩桓\n",
    "\n",
    "Student ID: M1244021 \n",
    "\n",
    "GitHub ID: Sohuan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM2024-Lab1-Master](https://github.com/didiersalazar/DM2024-Lab1-Master.git) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://huggingface.co/datasets/Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data). The dataset contains a `sentiment` and `comment` columns, with the sentiment labels being: 'nostalgia' and 'not nostalgia'. Read the specificiations of the dataset for background details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Scikit-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Note that for the TF-IDF features you might need to use other type of NB classifier different than the one in the Master Notebook. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be handled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/didiersalazar/DM2024-Lab1-Master/blob/main/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (October 27th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises in  DM2024-Lab1-Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 2 (take home):** \n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their [documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer here\n",
    "sci_med_subject_df = X[(X['category_name'] == 'sci.med') & (X['text'].str.contains('Subject'))]\n",
    "print(sci_med_subject_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 5 (take home)** \n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.   \n",
    "$Hint$ :  why `.isnull()` didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "#In the dictionary list NA_dict, strings like 'NaN' and 'None'(id C and D) are used to represent missing values. These strings will be treated as ordinary strings instead of NaN values ​​in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes from the `X` dataframe to the `X_sample` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# X_sample only contains 1000 randomly sampled rows of data from X, not the entire data.\n",
    "# Due to random sampling, the order of data rows in X_sample may be different from that in X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 8 (take home):** \n",
    "We can also do a side-by-side comparison of the distribution between the two datasets, but maybe you can try that as an excerise. Below we show you an snapshot of the type of chart we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "X_counts = X.category_name.value_counts()\n",
    "X_sample_counts = X_sample.category_name.value_counts()\n",
    "\n",
    "df_counts = pd.DataFrame({'X': X_counts, 'X_sample': X_sample_counts}).fillna(0)\n",
    "\n",
    "df_counts = df_counts.loc[X_counts.index]\n",
    "\n",
    "df_counts.plot(kind='bar', \n",
    "               figsize=(10, 5), \n",
    "               width=0.3,\n",
    "               color=['steelblue', 'darkorange'])\n",
    "plt.title('Category Distribution')\n",
    "plt.ylim([0, max(df_counts.max()) * 1.1])  # 動態設置 y 軸範圍\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(['X', 'X_sample'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 10 (take home):**\n",
    "We said that the `1` at the beginning of the fifth record represents the `00` term. Notice that there is another 1 in the same record. Can you provide code that can verify what word this 1 represents from the vocabulary. Try to do this as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "record = X_counts[4].toarray().flatten()  # 獲取第五筆記錄並轉換為密集數組\n",
    "\n",
    "one_indices = np.where(record == 1)[0]\n",
    "\n",
    "second_one_index = one_indices[1]\n",
    "\n",
    "count_vect.get_feature_names_out()[second_one_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **>>> Exercise 11 (take home):** \n",
    "From the chart above, we can see how sparse the term-document matrix is; i.e., there is only one terms with **FREQUENCY** of `1` in the subselection of the matrix. By the way, you may have noticed that we only selected 20 articles and 20 terms to plot the histrogram. As an excersise you can try to modify the code above to plot the entire term-document matrix or just a sample of it. How would you do this efficiently? Remember there is a lot of words in the vocab. Report below what methods you would use to get a nice and useful visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "doc_frequencies = X_counts.sum(axis=1).A1   \n",
    "term_frequencies = X_counts.sum(axis=0).A1  \n",
    "min_doc_frequency = 5 \n",
    "min_term_frequency = 5 \n",
    "\n",
    "X_filtered = X_counts[(doc_frequencies >= min_doc_frequency), :]\n",
    "X_filtered = X_filtered[:, (term_frequencies >= min_term_frequency)]\n",
    "plot_z = X_filtered[0:20, 0:20].toarray()\n",
    "\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(9, 7))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",fmt=\"d\",\n",
    "                 vmin=0, vmax=1, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 12 (take home):**\n",
    "If you want a nicer interactive visualization here, I would encourage you try to install and use plotly to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "terms = count_vect.get_feature_names_out()[:300]\n",
    "frequencies = term_frequencies[:300]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=terms,\n",
    "        y=frequencies,\n",
    "        text=frequencies,\n",
    "        textposition='auto'\n",
    "    )\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Term Frequencies of Top 300 Terms\",\n",
    "    xaxis_title=\"Terms\",\n",
    "    yaxis_title=\"Frequencies\",\n",
    "    xaxis_tickangle=-90, \n",
    "    width=1500, \n",
    "    height=500         \n",
    ")\n",
    "pyo.init_notebook_mode()  \n",
    "pyo.iplot(fig)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 13 (take home):** \n",
    "The chart above only contains 300 vocabulary in the documents, and it's already computationally intensive to both compute and visualize. Can you efficiently reduce the number of terms you want to visualize as an exercise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "\n",
    "top_n = 100\n",
    "threshold = np.sort(term_frequencies)[-top_n]\n",
    "\n",
    "high_freq_terms = count_vect.get_feature_names_out()[term_frequencies >= threshold]\n",
    "high_freq_frequencies = term_frequencies[term_frequencies >= threshold]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "g = sns.barplot(x=high_freq_terms, y=high_freq_frequencies, ax=ax)\n",
    "\n",
    "g.set_xticklabels(high_freq_terms, rotation=90)\n",
    "\n",
    "g.set_title('Top 100 High-Frequency Terms')\n",
    "g.set_xlabel('Terms')\n",
    "g.set_ylabel('Frequencies')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 14 (take home):** \n",
    "Additionally, you can attempt to sort the terms on the `x-axis` by frequency instead of in alphabetical order. This way the visualization is more meaninfgul and you will be able to observe the so called [long tail](https://en.wikipedia.org/wiki/Long_tail) (get familiar with this term since it will appear a lot in data mining and other statistics courses). see picture below\n",
    "\n",
    "![alt txt](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Long_tail.svg/1000px-Long_tail.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "terms = count_vect.get_feature_names_out()\n",
    "frequencies = term_frequencies\n",
    "\n",
    "term_freq_dict = dict(zip(terms, frequencies))\n",
    "sorted_terms = sorted(term_freq_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "top_n = 100 \n",
    "top_terms, top_frequencies = zip(*sorted_terms[:top_n])\n",
    "\n",
    "plt.figure(figsize=(20, 10)) \n",
    "g = sns.barplot(x=top_terms, y=top_frequencies)\n",
    "g.set_xticklabels(top_terms, rotation=90)\n",
    "g.set_title(f'Top {top_n} Term Frequencies')\n",
    "g.set_xlabel('Terms')\n",
    "g.set_ylabel('Frequencies')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 15 (take home):** \n",
    "You can copy the code from the previous exercise and change the 'term_frequencies' variable for the 'term_frequencies_log', comment about the differences that you observe and talk about other possible insights that we can get from a log distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# The frequency graph shows that there are a few extremely high-frequency words in the text, and these words stand out significantly on the graph; while the term_frequencies_log graph shows that more words have higher frequencies and are more evenly distributed. In terms of visual effects, the graph of term_frequencies_log makes it easier for observers to understand the overall occurrence of words in the text, while the graph of frequencies highlights the high-frequency characteristics of a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 16 (take home):** \n",
    "Review the words that were filtered in each category and comment about the differences and similarities that you can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# similarity\n",
    "# Negativity and negative emotions: Many filtered words have negative or negative connotations, such as \"disqualified\", \"disproving\", \"disgust\", \"disguised\", etc. These words imply something related to doubt, questioning or dissatisfaction emotions. In the category of \"alt.atheism\", such words may represent questioning and criticism of certain beliefs or concepts.\n",
    "# Personal-level vocabulary: Some words, such as \"personality\", \"personalities\", \"personify\", and \"persuade\", show the interpersonal interaction or influence of opinions in discussions in this category. These words may point to people in the \"alt.atheism\" category trying to influence others or express personal opinions.\n",
    "\n",
    "# difference\n",
    "# Political and Social Vocabulary: Words such as \"dissidents\", \"disenfranchisement\", \"pillage\" and \"plundered\" may be more suitable for political or social justice topics, expressing feelings of resistance or social injustice, which are found in \"alt.atheism\" may represent atheists’ dissatisfaction with power or institutions.\n",
    "# Scientific and philosophical concepts: Words such as \"discrepancy,\" \"discrete,\" and \"diverge\" are often used to describe differences and disagreements in opinions or beliefs, consistent with the arguments that atheists might challenge or analyze religious beliefs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 17 (take home):** \n",
    "Implement the FAE Top-K and MaxFPGrowth algorithms from the PAMI library to analyze the 'comp.graphics' category in our processed database. **Only implement the mining part of the algorithm and display the resulting patterns**, like we did with the FPGrowth algorithm after creating the new databases. For the FAE Top-K, run trials with k values of 500, 1000, and 1500, recording the runtime for each. For MaxFPGrowth, test minimum support thresholds of 3, 6, and 9, noting the runtime for these settings as well. Compare the patterns these algorithms extract with those from the previously implemented FPGrowth algorithm. Document your findings, focusing on differences and similarities in the outputs and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Here\n",
    "\n",
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "minSup=6\n",
    "obj5 = alg.FPGrowth(iFile='td_freq_db_comp_graphics.csv', minSup=minSup)\n",
    "obj5.mine()\n",
    "frequentPatternsDF_comp_graphics= obj5.getPatternsAsDataFrame()\n",
    "print('Total No of patterns: ' + str(len(frequentPatternsDF_comp_graphics))) #print the total number of patterns\n",
    "print('Runtime: ' + str(obj5.getRuntime())) #measure the runtime\n",
    "\n",
    "obj5.save('freq_patterns_comp_graphics_minSup6.txt') #save the patterns\n",
    "frequentPatternsDF_comp_graphics\n",
    "\n",
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "minSup=9\n",
    "obj5 = alg.FPGrowth(iFile='td_freq_db_comp_graphics.csv', minSup=minSup)\n",
    "obj5.mine()\n",
    "frequentPatternsDF_comp_graphics= obj5.getPatternsAsDataFrame()\n",
    "print('Total No of patterns: ' + str(len(frequentPatternsDF_comp_graphics))) #print the total number of patterns\n",
    "print('Runtime: ' + str(obj5.getRuntime())) #measure the runtime\n",
    "\n",
    "obj5.save('freq_patterns_comp_graphics_minSup9.txt') #save the patterns\n",
    "frequentPatternsDF_comp_graphics\n",
    "\n",
    "from PAMI.frequentPattern.basic import FPGrowth as alg\n",
    "minSup=3\n",
    "obj5 = alg.FPGrowth(iFile='td_freq_db_comp_graphics.csv', minSup=minSup)\n",
    "obj5.mine()\n",
    "frequentPatternsDF_comp_graphics= obj5.getPatternsAsDataFrame()\n",
    "print('Total No of patterns: ' + str(len(frequentPatternsDF_comp_graphics))) #print the total number of patterns\n",
    "print('Runtime: ' + str(obj5.getRuntime())) #measure the runtime\n",
    "\n",
    "obj5.save('freq_patterns_comp_graphics_minSup3.txt') #save the patterns\n",
    "frequentPatternsDF_comp_graphics\n",
    "\n",
    "# Answer Here\n",
    "# Reducing minSup will result in a large increase in the number of patterns, which means that the diversity of the data set will be higher, but may introduce many redundant or unimportant patterns.\n",
    "# The running time increases with the number of patterns, especially with minSup=3.\n",
    "# When analyzing the results, the practical application scenarios of the patterns should be considered to determine whether more patterns are needed or to focus on fewer but more representative patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 18 (take home):\n",
    "Please try to reduce the dimension to 3, and plot the result use 3-D plot. Use at least 3 different angle (camera position) to check your result and describe what you found.\n",
    "\n",
    "$Hint$: you can refer to Axes3D in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "col = ['coral', 'blue', 'black', 'orange']\n",
    "categories = X['category_name'].unique() \n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10), subplot_kw={'projection': '3d'})  # 3D subplots\n",
    "fig.suptitle('PCA, t-SNE, and UMAP Comparison in 3D')\n",
    "\n",
    "def plot_3d_scatter(ax, X_reduced, title, camera_angles):\n",
    "    for c, category in zip(col, categories):\n",
    "        xs = X_reduced[X['category_name'] == category][:, 0]\n",
    "        ys = X_reduced[X['category_name'] == category][:, 1]\n",
    "        zs = X_reduced[X['category_name'] == category][:, 2]\n",
    "        ax.scatter(xs, ys, zs, c=c, marker='o', label=category)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    for angle in camera_angles:\n",
    "        ax.view_init(*angle)\n",
    "\n",
    "camera_angles = [(30, 30), (60, 30), (90, 30)]\n",
    "\n",
    "plot_3d_scatter(axes[0], X_pca_aug, 'PCA', camera_angles)\n",
    "plot_3d_scatter(axes[1], X_tsne_aug, 't-SNE', camera_angles)\n",
    "plot_3d_scatter(axes[2], X_umap_aug, 'UMAP', camera_angles)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> **Exercise 19 (take home):**\n",
    "Try to generate the binarization using the `category_name` column instead. Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "mlb.fit(X.category_name)\n",
    "X['bin_category1'] = mlb.transform(X['category_name']).tolist()\n",
    "X[0:9]\n",
    "# It also work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Jason\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jason\\.cache\\huggingface\\hub\\datasets--Senem--Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 1500/1500 [00:00<00:00, 22177.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using TF-IDF features: 0.8333333333333334\n",
      "Accuracy using word frequency features: 0.96\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"Senem/Nostalgic_Sentiment_Analysis_of_YouTube_Comments_Data\")\n",
    "comments = ds['train']['comment']\n",
    "sentiments = ds['train']['sentiment']\n",
    "\n",
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(comments)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, sentiments, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier using TF-IDF features\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of test data using TF-IDF features\n",
    "y_pred_tfidf = nb_tfidf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the TF-IDF classifier\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# Create a simple Naive Bayes classifier using word frequency features\n",
    "X_freq = vectorizer.transform(comments)\n",
    "nb_freq = MultinomialNB()\n",
    "nb_freq.fit(X_freq, sentiments)\n",
    "\n",
    "# Predict the sentiment of test data using word frequency features\n",
    "y_pred_freq = nb_freq.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the word frequency classifier\n",
    "accuracy_freq = accuracy_score(y_test, y_pred_freq)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Accuracy using TF-IDF features:\", accuracy_tfidf)\n",
    "print(\"Accuracy using word frequency features:\", accuracy_freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
